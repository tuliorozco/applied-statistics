# K-Nearest Neighbors (KNN): Predicción del Éxito Académico y Riesgo de Deserción en Estudiantes Universitarios

$$
\begin{align}
\textit{Leyniker Escobar Brache, Carlos Hodwalker Martínez, Tulio Orozco Torres} \\
\textit{Maestría en Estadística Aplicada, Universidad del Norte} \\
\small{\textit{Agosto 30 de 2025}}
\end{align}
$$


#### Proyecto Integrador de Aprendizaje Automático

## Resumen


## Algoritmos a Evaluar

A continuación, se define el alcance de los algoritmos a evaluar y que son del interés de este artículo.

* `KNeighborsClassifier`
* `LogisticRegression`
* `DecisionTreeClassifier`
* `Random Forest`
* `XGBoost`
* `SVC` (Support Vector Machine/Máquinas de Vectores de Soporte)
* `MLP` (Multilayer Perceptron/Red Neuronal Multicapa)
* `MOE Conformal Transformer` (Mixture of Experts-Conformal Prediction Transformed Based)

La propuesta de este documento es la de hacer un *benchmark* entre todos los algoritmos citados y compararlos, especialmente, con el propuesto: `MOE Conformal Transformer`. Sin embargo, antes de la implementación o entrenamiento se abordará la fundamentacieon del algoritmo propuesto.

### Modelo Propuesto: `Mixture of Experts-Conformal Prediction-Transformed Based (MOE Conformal Transformer)`
Para entender qué es `MoE-Conformal Transformer`, cómo funciona, sus características, entre otros aspectos así como el impacto que tiene su aplicación reciente en *deep learning* (como en Large Language Models o LLMs), se presenta una vista general de los 3 componentes que lo integran:

* `Mixture of Experts` o Mezcla de Expertos.
* `Conformal Prediction` o Predicción Conforme.
* `Transformers`

### ¿Qué es `Mixture of Experts`?
### ¿Qué es `Conformal Prediction`?
### ¿Qué son los `Transformers`?